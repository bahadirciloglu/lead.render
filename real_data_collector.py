"""
Real Data Collector Service
Ger√ßek veri toplama servisi - Mockup veri kullanmaz
"""

import asyncio
import httpx
import json
import os
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from real_data_config import REAL_DATA_SOURCES, DATA_QUALITY_STANDARDS
import time


class RealDataCollector:
    """Ger√ßek veri toplama servisi"""
    
    def __init__(self):
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        
        # Rate limiting - t√ºm servisler i√ßin list format
        self.request_counts = {
            "openrouter": [],     # Timestamp listesi
            "google": []          # Google Gemini i√ßin
        }
        
        # Daily quotas
        self.daily_quotas = {
            "openrouter": 1000,    # OpenRouter free tier
            "google": 1000         # Google Gemini free tier
        }
        

    
    async def collect_startup_data(self, filters: Dict, user_message: str = "") -> Dict:
        """Ger√ßek veri kaynaklarƒ±ndan startup verisi topla"""
        
        results = {
            "status": "processing",
            "total_companies": 0,
            "llm_analysis": [],
            "data_sources": [],
            "error": None
        }
        
        try:
            # 1. LLM Analysis - Ge√ßici olarak API key kontrol√ºn√º devre dƒ±≈üƒ± bƒ±rak
            print("üîç LLM Analysis ba≈ülatƒ±lƒ±yor (API key kontrol√º devre dƒ±≈üƒ±)...")
            llm_results = await self._analyze_with_llm(filters, [], user_message)
            results["llm_analysis"] = llm_results
            results["data_sources"].append("Multi-LLM Models")
            results["total_companies"] += len(llm_results)  # LLM sonu√ßlarƒ±nƒ± ≈üirket olarak say
            
        except Exception as e:
            results["error"] = str(e)
            results["status"] = "failed"
        
        results["status"] = "success"
        return results
    
    async def _analyze_with_llm(self, filters: Dict, web_results: List[Dict], user_message: str = "") -> List[Dict]:
        """Parallel Multi-LLM sistemi ile analiz yap"""
        
        print(f"üîç Parallel Multi-LLM Analysis ba≈ülatƒ±lƒ±yor...")
        print(f"üìä Filters: {filters}")
        print(f"üí¨ User Message: {user_message}")
        print(f"üåê Web results count: {len(web_results)}")
        
        # 2 LLM modeli paralel olarak √ßalƒ±≈ütƒ±r
        tasks = [
            self._try_google_gemini(filters, web_results, user_message),       # Google Gemini (en √ºstte)
            self._try_openrouter_gpt_oss(filters, web_results, user_message),  # GPT-OSS-20B:free
        ]
        
        print(f"üöÄ 2 LLM modeli paralel olarak √ßalƒ±≈ütƒ±rƒ±lƒ±yor...")
        
        # T√ºm sonu√ßlarƒ± bekle
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Ba≈üarƒ±lƒ± sonu√ßlarƒ± topla
        all_llm_responses = []  # ‚úÖ LLM yanƒ±tlarƒ± i√ßin ayrƒ± liste
        successful_models = []
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"‚ùå Model {i+1} error: {result}")
            elif result and len(result) > 0:
                model_names = ["Google Gemini", "GPT-OSS-20B"]
                print(f"‚úÖ {model_names[i]} ba≈üarƒ±lƒ±: {len(result)} responses")
                all_llm_responses.extend(result)  # ‚úÖ LLM yanƒ±tlarƒ± ekle
                successful_models.append(model_names[i])
        
        print(f"üéØ Toplam {len(all_llm_responses)} LLM yanƒ±tƒ± alƒ±ndƒ±")  # ‚úÖ Daha a√ßƒ±k mesaj
        print(f"üèÜ Ba≈üarƒ±lƒ± modeller: {', '.join(successful_models)}")
        
        return all_llm_responses  # ‚úÖ LLM yanƒ±tlarƒ± d√∂nd√ºr
    
    async def _try_openrouter_gpt_oss(self, filters: Dict, web_results: List[Dict], user_message: str = "") -> List[Dict]:
        """OpenRouter GPT-OSS-20B ile analiz yap"""
        
        if not self._check_rate_limit("openrouter"):
            print("‚ùå OpenRouter rate limit exceeded - daily quota reached")
            return [{
                "model": "GPT-OSS-20B",
                "llm_response": "Rate limit exceeded",
                "status": "error",
                "timestamp": datetime.now().isoformat()
            }]
        
        # Prompt yerine user message kullan
        message_content = user_message if user_message.strip() else "Hello"
        
        try:
            print(f"üöÄ OpenRouter GPT-OSS-20B API √ßaƒürƒ±sƒ±...")
            async with httpx.AsyncClient() as client:
                headers = {
                    "Authorization": f"Bearer {self.openrouter_api_key}",
                    "Content-Type": "application/json"
                }
                
                data = {
                    "model": "openai/gpt-oss-20b:free",  # OpenRouter model (Free)
                    "messages": [{"role": "user", "content": message_content}],
                    "max_tokens": 1000
                }
                
                response = await client.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    print(f"ü§ñ GPT-OSS-20B Response preview: {content[:300]}...")
                    # Parsing kaldƒ±rƒ±ldƒ± - direkt LLM yanƒ±tƒ± d√∂nd√ºr
                    self._increment_request_count("openrouter")
                    return [{
                        "model": "GPT-OSS-20B",
                        "llm_response": content,
                        "user_question": user_message,
                        "status": "success",
                        "timestamp": datetime.now().isoformat()
                    }]
                else:
                    print(f"‚ùå GPT-OSS-20B Error: {response.status_code}")
                    print(f"üìù Error Response: {response.text[:200]}...")
                    return [{
                        "model": "GPT-OSS-20B",
                        "llm_response": f"API Error: {response.status_code}",
                        "user_question": user_message,
                        "status": "error",
                        "timestamp": datetime.now().isoformat()
                    }]
                    
        except Exception as e:
            print(f"‚ùå GPT-OSS-20B error: {e}")
            return [{
                "model": "GPT-OSS-20B",
                "llm_response": f"Error: {str(e)}",
                "user_question": user_message,
                "status": "error",
                "timestamp": datetime.now().isoformat()
            }]
    
    async def _try_google_gemini(self, filters: Dict, web_results: List[Dict], user_message: str = "") -> List[Dict]:
        """Google Gemini ile analiz"""
        
        if not self._check_rate_limit("google"):
            print("‚ùå Google rate limit exceeded")
            return []
        
        # Prompt yerine user message kullan
        message_content = user_message if user_message.strip() else "Hello"
        
        try:
            print(f"üöÄ Google Gemini API √ßaƒürƒ±sƒ±...")
            async with httpx.AsyncClient() as client:
                headers = {
                    "Content-Type": "application/json"
                }
                
                data = {
                    "contents": [{
                        "parts": [{"text": message_content}]
                    }]
                }
                
                # Google Gemini API endpoint
                url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={self.google_api_key}"
                
                response = await client.post(
                    url,
                    headers=headers,
                    json=data,
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['candidates'][0]['content']['parts'][0]['text']
                    print(f"ü§ñ Google Gemini Response preview: {content[:300]}...")
                    # Parsing kaldƒ±rƒ±ldƒ± - direkt LLM yanƒ±tƒ± d√∂nd√ºr
                    self._increment_request_count("google")
                    return [{
                        "llm_response": content, 
                        "model": "Google Gemini", 
                        "user_question": user_message,
                        "status": "success"
                    }]
                else:
                    print(f"‚ùå Google Gemini Error: {response.status_code}")
                    print(f"üìù Error Response: {response.text[:200]}...")
                    return [{
                        "llm_response": f"API Error: {response.status_code}", 
                        "model": "Google Gemini", 
                        "user_question": user_message,
                        "status": "error"
                    }]
                    
        except Exception as e:
            print(f"‚ùå Google Gemini error: {e}")
            return [{
                "llm_response": f"Error: {str(e)}", 
                "model": "Google Gemini", 
                "user_question": user_message,
                "status": "error"
            }]
    
    def _create_llm_prompt(self, filters: Dict, web_results: List[Dict]) -> str:
        """User message'ƒ± direkt g√∂nder - prompt yok"""
        # Prompt kaldƒ±rƒ±ldƒ± - sadece user message kullan
        return ""
    
    # _parse_llm_response metodu kaldƒ±rƒ±ldƒ± - gereksiz
    

    
    def _check_rate_limit(self, service: str) -> bool:
        """Rate limit kontrol√º - t√ºm servisler i√ßin tutarlƒ±"""
        current_time = time.time()
        
        # Servis i√ßin rate limit ayarlarƒ±
        if service == "openrouter":
            daily_limit = 1000  # OpenRouter: g√ºnl√ºk 1000 request
        elif service == "google":
            daily_limit = 1000 # Google Gemini: g√ºnl√ºk 1000 request
        else:
            return True  # Bilinmeyen servis i√ßin limit yok
        
        # 24 saatlik window
        window_start = current_time - 86400
        
        # Servis i√ßin request listesi yoksa olu≈ütur
        if service not in self.request_counts:
            self.request_counts[service] = []
        
        # Window i√ßindeki request sayƒ±sƒ±nƒ± hesapla
        requests_in_window = [
            req_time for req_time in self.request_counts[service]
            if isinstance(req_time, (int, float)) and req_time > window_start
        ]
        
        # Limit kontrol√º
        can_make_request = len(requests_in_window) < daily_limit
        
        if not can_make_request:
            print(f"‚ö†Ô∏è Rate limit exceeded for {service}: {len(requests_in_window)}/{daily_limit}")
        
        return can_make_request
    
    def _increment_request_count(self, service: str):
        """Request count'u artƒ±r"""
        if service not in self.request_counts:
            self.request_counts[service] = []
        
        self.request_counts[service].append(time.time())
        
        # Eski request'leri temizle (24 saatten eski)
        current_time = time.time()
        self.request_counts[service] = [
            req_time for req_time in self.request_counts[service]
            if current_time - req_time < 86400
        ]
    
    def get_collection_status(self) -> Dict:
        """Veri toplama durumunu getir - yeni rate limiting sistemi ile"""
        
        current_time = time.time()
        window_start = current_time - 86400
        
        def get_service_status(service_name: str, daily_limit: int) -> Dict:
            if service_name not in self.request_counts:
                return {
                    "enabled": False,
                    "quota_used": 0,
                    "quota_limit": daily_limit,
                    "quota_remaining": daily_limit
                }
            
            # 24 saatlik window i√ßindeki request sayƒ±sƒ±
            requests_in_window = [
                req_time for req_time in self.request_counts[service_name]
                if isinstance(req_time, (int, float)) and req_time > window_start
            ]
            
            quota_used = len(requests_in_window)
            quota_remaining = max(0, daily_limit - quota_used)
            
            return {
                "enabled": True,
                "quota_used": quota_used,
                "quota_limit": daily_limit,
                "quota_remaining": quota_remaining
            }
        
        return {
            "openrouter": get_service_status("openrouter", 1000),
            "google_gemini": get_service_status("google", 1000)
        } 